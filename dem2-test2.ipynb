{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5015f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = r\"mmash\\mmash\\MMASH\\DataPaper\"\n",
    "\n",
    "activity_labels = {\n",
    "    1: \"Sleeping\",\n",
    "    2: \"Lying down\",\n",
    "    3: \"Sitting\",\n",
    "    4: \"Light movement\",\n",
    "    5: \"Medium movement\",\n",
    "    6: \"Heavy movement\",\n",
    "    7: \"Eating\",\n",
    "    8: \"Small screen usage\",\n",
    "    9: \"Large screen usage\",\n",
    "    10: \"Caffeinated drinks\",\n",
    "    11: \"Smoking\",\n",
    "    12: \"Alcohol consumption\"\n",
    "}\n",
    "\n",
    "def load_data(user_id):\n",
    "    \"\"\"\n",
    "    Reads Activity.csv and Actigraph.csv for a single user.\n",
    "    Returns two dataframes: df_acti, df_actig\n",
    "    \"\"\"\n",
    "    user_folder = os.path.join(DATA_DIR, f\"user_{user_id}\")\n",
    "    activity_file = os.path.join(user_folder, \"Activity.csv\")\n",
    "    actigraph_file = os.path.join(user_folder, \"Actigraph.csv\")\n",
    "\n",
    "    df_acti = pd.read_csv(activity_file)\n",
    "    df_actig = pd.read_csv(actigraph_file)\n",
    "    return df_acti, df_actig\n",
    "\n",
    "# Example: load for a single user\n",
    "df_acti_1, df_actig_1 = load_data(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859c5614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Times and Label Each Second\n",
    "def parse_times(df_acti, df_actig):\n",
    "    \"\"\"\n",
    "    Create new datetime columns in both dataframes.\n",
    "    \"\"\"\n",
    "    # Convert Day + \"HH:MM\" -> datetime\n",
    "    def make_datetime_activity(day_number, hhmm_str, base_date=datetime.date(2025,1,1)):\n",
    "        day_offset = datetime.timedelta(days=int(day_number) - 1)\n",
    "        if hhmm_str == \"24:00\":\n",
    "            # Roll over to next day at 00:00\n",
    "            day_offset += datetime.timedelta(days=1)\n",
    "            hhmm_str = \"00:00\"\n",
    "        t = datetime.datetime.strptime(hhmm_str, \"%H:%M\").time()\n",
    "        return datetime.datetime.combine(base_date + day_offset, t)\n",
    "\n",
    "    df_acti = df_acti.dropna(subset=[\"Start\", \"End\"]).copy()\n",
    "    df_acti[\"Start_dt\"] = df_acti.apply(\n",
    "        lambda row: make_datetime_activity(row[\"Day\"], row[\"Start\"]), axis=1\n",
    "    )\n",
    "    df_acti[\"End_dt\"] = df_acti.apply(\n",
    "        lambda row: make_datetime_activity(row[\"Day\"], row[\"End\"]), axis=1\n",
    "    )\n",
    "\n",
    "    # Convert day + \"HH:MM:SS\" -> datetime\n",
    "    def make_datetime_actigraph(day_number, hhmmss_str, base_date=datetime.date(2025,1,1)):\n",
    "        day_offset = datetime.timedelta(days=int(day_number) - 1)\n",
    "        t = datetime.datetime.strptime(hhmmss_str, \"%H:%M:%S\").time()\n",
    "        return datetime.datetime.combine(base_date + day_offset, t)\n",
    "\n",
    "    df_actig = df_actig.copy()\n",
    "    df_actig[\"Datetime\"] = df_actig.apply(\n",
    "        lambda row: make_datetime_actigraph(row[\"day\"], row[\"time\"]), axis=1\n",
    "    )\n",
    "\n",
    "    return df_acti, df_actig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299b1dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f321b1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ActivityCode        ActivityLabel  TotalMinutes\n",
      "0              1             Sleeping         318.0\n",
      "1              2           Lying down        4321.0\n",
      "2              3              Sitting        4812.0\n",
      "3              4       Light movement          87.0\n",
      "4              5      Medium movement         460.0\n",
      "5              6       Heavy movement        2059.0\n",
      "6              7               Eating        3209.0\n",
      "7              8   Small screen usage        1262.0\n",
      "8              9   Large screen usage         385.0\n",
      "9             10   Caffeinated drinks          65.0\n",
      "10             0          Unknown (0)        6038.0\n",
      "11            12  Alcohol consumption         227.0\n",
      "12            11              Smoking         160.0\n"
     ]
    }
   ],
   "source": [
    "all_activity_counts = {}\n",
    "for user_id in range(1, 23):\n",
    "    try:\n",
    "        df_acti, df_actig = load_data(user_id)\n",
    "        df_acti, df_actig = parse_times(df_acti, df_actig)\n",
    "\n",
    "        df_acti = df_acti.dropna(subset=[\"Start\", \"End\", \"Activity\"])\n",
    "        df_acti = df_acti.dropna()\n",
    "        df_acti = df_acti[df_acti[\"End_dt\"] >= df_acti[\"Start_dt\"]]\n",
    "        \n",
    "\n",
    "        # Sum the duration for each activity\n",
    "        df_acti[\"Duration_minutes\"] = (\n",
    "            df_acti[\"End_dt\"] - df_acti[\"Start_dt\"]\n",
    "        ).dt.total_seconds() / 60.0\n",
    "\n",
    "        activity_duration = df_acti.groupby(\"Activity\")[\"Duration_minutes\"].sum()\n",
    "        for act_code, duration in activity_duration.items():\n",
    "            all_activity_counts[act_code] = all_activity_counts.get(act_code, 0) + duration\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass  # in case any user is missing\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df_distribution = pd.DataFrame([\n",
    "    {\"ActivityCode\": k, \n",
    "     \"ActivityLabel\": activity_labels.get(k, f\"Unknown ({k})\"),\n",
    "     \"TotalMinutes\": v}\n",
    "    for k, v in all_activity_counts.items()\n",
    "])\n",
    "print(df_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c16c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-range HR samples: 1\n"
     ]
    }
   ],
   "source": [
    "# Quick check for out-of-range HR\n",
    "invalid_hr = df_actig_1[(df_actig_1[\"HR\"] < 20) | (df_actig_1[\"HR\"] > 220)]\n",
    "print(\"Out-of-range HR samples:\", len(invalid_hr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b409dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_user_raw(user_id):\n",
    "    \"\"\"\n",
    "    Preprocess raw actigraph data for a user:\n",
    "    - Load data\n",
    "    - Parse datetime columns\n",
    "    - Interpolate missing sensor values\n",
    "    \"\"\"\n",
    "    df_acti, df_actig = load_data(user_id)\n",
    "    df_acti, df_actig = parse_times(df_acti, df_actig)\n",
    "    \n",
    "\n",
    "    sensor_cols = [\"Axis1\", \"Axis2\", \"Axis3\"]\n",
    "    df_actig[sensor_cols] = df_actig[sensor_cols].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    return df_acti, df_actig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_dict = {}\n",
    "\n",
    "for user_id, segments in all_user_segments.items():\n",
    "    X_user = np.array(segments)\n",
    "    y_user = np.array([user_id] * len(segments))  # or use activity label here\n",
    "    user_data_dict[user_id] = (X_user, y_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Fit on all data\n",
    "global_le = LabelEncoder()\n",
    "all_labels = np.concatenate([y for _, y in user_data_dict.values()])\n",
    "global_le.fit(all_labels)\n",
    "\n",
    "num_classes = len(global_le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loso_evaluate_model(create_model_fn, user_data_dict, label_type=\"user\", epochs=30):\n",
    "    \"\"\"\n",
    "    Performs Leave-One-Subject-Out (LOSO) cross-validation.\n",
    "    user_data_dict: {user_id: (X, y)} ‚Äî where X is raw windows, y is categorical labels\n",
    "    label_type: \"user\" or \"activity\" (for printing purposes)\n",
    "    \"\"\"\n",
    "    all_accuracies = []\n",
    "    all_f1_scores = []\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "\n",
    "    user_ids = sorted(user_data_dict.keys())\n",
    "\n",
    "    for test_user in user_ids:\n",
    "        print(f\"\\nüîÅ Testing on User {test_user} (LOSO)\")\n",
    "\n",
    "        # Prepare train/test sets\n",
    "        X_test, y_test = user_data_dict[test_user]\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for user_id, (X_u, y_u) in user_data_dict.items():\n",
    "            if user_id != test_user:\n",
    "                X_train.extend(X_u)\n",
    "                y_train.extend(y_u)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        X_test = np.array(X_test)\n",
    "        y_test = np.array(y_test)\n",
    "\n",
    "        # Encode using global encoder\n",
    "        y_train_enc = global_le.transform(y_train)\n",
    "        y_test_enc = global_le.transform(y_test)\n",
    "\n",
    "        # One-hot encode\n",
    "        y_train_cat = to_categorical(y_train_enc, num_classes=num_classes)\n",
    "        y_test_cat = to_categorical(y_test_enc, num_classes=num_classes)\n",
    "\n",
    "        model = create_model_fn(input_shape=X_train.shape[1:], num_classes=y_train_cat.shape[1])\n",
    "        early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train_cat,\n",
    "            epochs=10,\n",
    "            batch_size=64,\n",
    "            validation_data=(X_test, y_test_cat),\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Evaluation\n",
    "        loss, accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "        y_true_labels = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "        report = classification_report(y_true_labels, y_pred_labels, output_dict=True, zero_division=0)\n",
    "        f1 = report['weighted avg']['f1-score']\n",
    "        precision = report['weighted avg']['precision']\n",
    "        recall = report['weighted avg']['recall']\n",
    "\n",
    "        print(f\"‚úÖ Accuracy: {accuracy:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "        all_accuracies.append(accuracy)\n",
    "        all_f1_scores.append(f1)\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "    print(\"\\nüìä LOSO Summary:\")\n",
    "    print(f\"Avg Accuracy:  {np.mean(all_accuracies):.4f}\")\n",
    "    print(f\"Avg F1 Score:  {np.mean(all_f1_scores):.4f}\")\n",
    "    print(f\"Avg Precision: {np.mean(all_precisions):.4f}\")\n",
    "    print(f\"Avg Recall:    {np.mean(all_recalls):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
